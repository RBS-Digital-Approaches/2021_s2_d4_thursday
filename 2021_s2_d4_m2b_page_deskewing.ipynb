{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021_s2_d4_m2b_page_deskewing.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMlr8rq2AHbz4WDYDYi541H"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wh2j0fLvij2q"},"source":["# Deskewing pages\n","This notebook was designed to introduce one method for automatically deskewing page images in preparation for OCR. For our purposes in this iteration of the class, the details of the process are less important than seeing an example of the ways that images can be computationally changed after initial photography: when you're looking at a digital surrogate, you're seeing an image that has likely been through several processes that attempt to optimize it for the task at hand without a human having to check it at each step of the way.\n","\n","For today, there's **really** no need to try to figure out any of the details of the code: the more important thing is to get a general sense of what's happening, and then observe the differences that those changes make to the image in the end.\n","\n","The code in this notebook is drawn from a blog post by Leo Ertuna at [Becoming Human](https://becominghuman.ai/how-to-automatically-deskew-straighten-a-text-image-using-opencv-a0c30aed83df). This notebook breaks Ertuna's code up into interactive steps to show what's happening along the way."]},{"cell_type":"markdown","metadata":{"id":"EeYTN8gDMyb9"},"source":["### A note before we start: this may not be the only problem to solve\n","The code in this notebook assumes that the problem with the page image is that it's skewed and needs to straightened—this treats the page as a two-dimensional plane. \n","\n","That works pretty well if the images are of reasonably flat pages of the kind that we can often get from the sort of imaging labs that many libraries have. But books don't necessarily lay flat, and, depending on the condition of the binding, it may not always possible to flatten the pages for imaging. So the lines of text in some images will appear not just skewed, but actually curved, due to the curvature of the pages. And pages in a book can curl in a number of ways all at once (recall, for instance how much more and how differently the pages in the middle of a thick book curl compared to pages at the beginning or end.)\n","\n","It's possible to reduce or eliminate the appearance of curvature in the lines of a page image incorporating some of techniques we'll see in this deskewing routine (but adding some others). That's a more complicated problem that we won't take on, but there's a great [blog post by Mark Zucker](https://mzucker.github.io/2016/08/15/page-dewarping.html) that walks through a solution. The blog post offers visualizations of what's happening at each step, so it's an instructive read even if you're not examining the details of the code. In the interests of time, I'll urge you **not** to examine the details of the code today, but to skim Zucker's description of his approach to the problem and look at the illustrations that visualize what the code is *doing*."]},{"cell_type":"markdown","metadata":{"id":"xhhbM2leRoE2"},"source":["## 1 - Connect to Google Drive, copy files, and install packages"]},{"cell_type":"code","metadata":{"id":"vr2ZLKaBb8Kc"},"source":["#Code cell #1\n","#Get access to Google Drive\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kM_B4qhhc4bp"},"source":["#Code cell #2\n","%cp -r /gdrive/MyDrive/L-100a/page_images.zip /content/page_images.zip\n","%cd /content/\n","!unzip page_images.zip\n","%cd /content/page_images/\n","!unzip penn_pr3732_t7_1730b.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5kdzNrKcbUL"},"source":["#Code cell #3\n","#Install IPyWidgets to provide widgets for experimenting with some variables later\n","import ipywidgets as widgets\n","from ipywidgets import interact\n","\n","#Import necessary Python packages for use in our code. \n","\n","#Note that opencv-python is installed by default in Google Colaboratory. If you \n","#were working in a different environment, you'd need to be sure it was installed\n","#using pip\n","\n","#(The second import is specific to Google Colaboratory and provides a workaround\n","#to get OpenCV's imshow command to work properly in a Colab notebook.\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aO7EsJLZR5K6"},"source":["## 2 - Opening the image\n","Run the next cell to get a drop-down list of different images for processing, all with varying levels of skewing. (You only need to run that cell once. Thereafter, you can change the image you're working with by choosing a different image from the select menu.)"]},{"cell_type":"code","metadata":{"id":"TAxKrLvwC6zw"},"source":["#Code cell #4\n","image_select = widgets.Dropdown(\n","    description='Choose image',\\\n","    options = ['PR3732_T7_1730b_body00' + i for i in ['04.tif', '11.tif', '13.tif', '21.tif', '36.tif', '63.tif', '78.tif', '82.tif']],\\\n","    value = 'PR3732_T7_1730b_body0004.tif',\n","    style={'description_width': 'initial'})\n","display(image_select)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rWyifXGpdPDR"},"source":["#Code cell #5\n","#Identify the skewed image and have OpenCV read it. (This can take a little \n","#while, so give it time to complete.)\n","\n","source_directory = '/content/page_images/penn_pr3732_t7_1730b/'\n","skewed_image = source_directory + image_select.value\n","im = cv2.imread(skewed_image, cv2.IMREAD_COLOR)\n","#Let's see what the image looks like: an excellent image, but a little skewed.\n","cv2_imshow(im)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S22E8xx2n42d"},"source":["## 3 - Manipulating the image\n","OCR software like Tesseract might well be able to handle an image like this, but recognition of text lines will be better if we can straighten it. Ertuna's script offers a nice example of a workflow for figuring out exactly *how* skewed the image is, then using that measurement to straighten the image. As we proceed, you'll see some of the ways that images that are good for *us* are not as useful for the computer, and vice versa."]},{"cell_type":"code","metadata":{"id":"JLPMgkleaPEW"},"source":["#Code cell #6\n","#Make a copy of the image\n","newImage = im.copy()\n","#Convery to grayscale\n","gray = cv2.cvtColor(newImage, cv2.COLOR_BGR2GRAY)\n","#Apply a Gaussian blur to reduce the effect of any noise in the image\n","blur = cv2.GaussianBlur(gray, (9, 9), 0)\n","#Convert the image to inverted black and white (i.e., white text on a \n","#black background). Note that Ertuna's script uses Otsu's method for \n","#thresholding to black and white.\n","thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n","cv2_imshow(thresh)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xplCmGRlS4J3"},"source":["### 3.a - Set some variables\n","Run the next cell to create a few sliders that will allow you to adjust the variables used in the next steps. (You only need to run that cell once. Thereafter, changing the sliders will change the values of the variables used in the subsequent cells.)"]},{"cell_type":"code","metadata":{"id":"WNEI3nnFnG_7"},"source":["#Code cell #7\n","kernel_width = widgets.IntSlider(description = 'Kernel width', \\\n","                                               min=10, max=50, step=5, value=30)\n","kernel_height = widgets.IntSlider(description='Kernel height', \\\n","                                                 min=1, max=10, step=1, value=5)\n","num_iterations = widgets.IntSlider(description='Iterations', min=1, \\\n","                      max=10, step=1, value=5)\n","display(kernel_width) \n","display(kernel_height)\n","display(num_iterations)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u7avDI3XfOOJ"},"source":["### 3.b - Now things start to get strange...\n","Ertuna implements a common approach that may seem counterintuitive at first: we're going to `dilate` the white pixels of the text until they run together to form solid blocks of white. \n","The next cell creates a set of sliders so you can play with the values that determine the size of the `kernel` used to dilate those pixels and the number of iterations for dilation (the defaults follow Ertuna's script). \n","NOTE: Don't re-run Code cell #7—that will just reset the values to their defaults. Instead, make any adjustments to the sliders and then run Code cell #8."]},{"cell_type":"code","metadata":{"id":"ga-SDNWCakMh"},"source":["#Code cell #8\n","#The kernel variable defines a shape to use for dilating the pixels: in this \n","#case, a rectangle of 30 pixels wide and 5 pixels high. These proportions ensure\n","# that the text will run together while more or less maintaining the vertical \n","#dimensions of the text lines. You could experiment with changing the x and y\n","#dimensions that are passed to cv2.MORPH_RECT to see how the output changes.\n","kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_width.value, kernel_height.value))\n","#We dilate the pixels using the shape defined by kernel, and perform the operation\n","#five times. You could try increasing or decreasing the number of iterations to\n","#see how the output changes.\n","dilate = cv2.dilate(thresh, kernel, iterations=num_iterations.value)\n","cv2_imshow(dilate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"43N1xuI7g7Op"},"source":["This cell finds the boundaries of the dilated white blocks that used to be our text lines and determines their contours. \n","I've made one adjustment to Ertuna's script here in using the `RETR_EXTERNAL` method rather than the `RETR_LIST` method that Ertuna used. Ertuna's method retrieves *all* contours that are detected, where `RETR_EXTERNAL` ignores contours that are found *within other contours*. Though I can't say I've tested it entirely systematically, this approach seems to do a better job of detecting text blocks in this eighteenth-century text, where what may be wandering in the baseline of the set type creates some gaps that end up being detected as contours."]},{"cell_type":"code","metadata":{"id":"dEyqVb6ha5VB"},"source":["#Code cell #9\n","#Determine contours\n","contours, hierarchy = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","#These next steps are not really part of the deskewing sequence. I've included \n","#them simply so we can see what's happening. We first create a color version\n","#of our black-and-white image, then draw a bright green line connecting the\n","#contour points so we can see the outlines of the detected shapes\n","show_contours = cv2.cvtColor(dilate, cv2.COLOR_BayerGR2RGB)\n","draw_contours = cv2.drawContours(show_contours, contours, -1, (115,255,105), 3)\n","cv2_imshow(draw_contours)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uy5WJ4ROiavU"},"source":["### 3.c - Finding the rectangle that fits this contour\n","Ertuna's script acts on only the largest of the detected areas on the not-unreasonable premise that the skew angle of the largest text block will be a good proxy for the skew angle of the entire page of text. (He notes, though, that other approaches are possible. One might find that the angle of a different block yielded better results, or the average of multiple blocks.)\n","\n","Having selected the largest contour, the code in the next cell then determines the smallest possible rectangle that could contain the entire contour using `minAreaRect`: basically, we're drawing a straight-sided box around the irregular contour of the text block.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mkkz_e_Ckh5V"},"source":["(The code in the following cell is, again, not really part of the deskewing procedure, but I have added it to show what's happening.)\n","\n","Actually, what `minAreaRect` produces is not exactly a rectangle, but rather some *instructions* for making a rectangle. We get:\n","- the x, y coordinates of the center point;\n","- the width and height of the rectangle; and\n","- the angle of the rectangle (for more on how `minAreaRect` treats this angle, see [this post at *The AI Learner*](https://theailearner.com/tag/cv2-minarearect/).)\n","\n","To actually draw the rectangle described by `minAreaRect`, the following cell uses OpenCV's `boxPoints` to get the corner points, then draws a series of lines to connect those corners."]},{"cell_type":"code","metadata":{"id":"YlqhgCKAdjz3"},"source":["#Code cell #10\n","#This sorts the set of contour points in reverse order: the contours of the \n","#largest area will be first in this sorted, which will come in handy in the next\n","#cell.\n","sorted_contours = sorted(contours, key = cv2.contourArea, reverse = True)\n","#Select the largest detected contour\n","largest_contour = sorted_contours[0]\n","#Determine the minimum-area rectangle that would contain that contour\n","largest_min_area_rect = cv2.minAreaRect(largest_contour)\n","print(largest_min_area_rect)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o1tU-RbJxJqw"},"source":["#Code cell #11\n","def draw_min_area_rect(cv2minimumarearectangle, base_image) :\n","  draw_min_area_rect = cv2.cvtColor(base_image, cv2.COLOR_BayerGR2RGB)\n","  if isinstance(cv2minimumarearectangle, list) == True :\n","    print(len(cv2minimumarearectangle))\n","    for rect in cv2minimumarearectangle :\n","      min_area_box = cv2.boxPoints(rect)\n","      min_area_box = np.int0(min_area_box)\n","      draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[0][0], min_area_box[0][1]), \\\n","                                    (min_area_box[1][0], min_area_box[1][1]), (0, 30, 255), 3)\n","      draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[1][0], min_area_box[1][1]), \\\n","                                    (min_area_box[2][0], min_area_box[2][1]), (0, 30, 255), 3)\n","      draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[2][0], min_area_box[2][1]), \\\n","                                    (min_area_box[3][0], min_area_box[3][1]), (0, 30, 255), 3)\n","      draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[3][0], min_area_box[3][1]), \\\n","                                    (min_area_box[0][0], min_area_box[0][1]), (0, 30, 255), 3)\n","      cv2.putText(draw_min_area_rect, str(rect[-1]), \n","                  (int(rect[0][0]) -100, int(rect[0][1])), cv2.FONT_HERSHEY_SIMPLEX, \n","                  1, (0, 30, 255, 255), 3)\n","  else :\n","    min_area_box = cv2.boxPoints(cv2minimumarearectangle)\n","    min_area_box = np.int0(min_area_box)\n","    draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[0][0], min_area_box[0][1]), \\\n","                                  (min_area_box[1][0], min_area_box[1][1]), (0, 30, 255), 3)\n","    draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[1][0], min_area_box[1][1]), \\\n","                                  (min_area_box[2][0], min_area_box[2][1]), (0, 30, 255), 3)\n","    draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[2][0], min_area_box[2][1]), \\\n","                                  (min_area_box[3][0], min_area_box[3][1]), (0, 30, 255), 3)\n","    draw_min_area_rect = cv2.line(draw_min_area_rect, (min_area_box[3][0], min_area_box[3][1]), \\\n","                                  (min_area_box[0][0], min_area_box[0][1]), (0, 30, 255), 3)\n","    cv2.putText(draw_min_area_rect, str(cv2minimumarearectangle[-1]), \n","                (int(cv2minimumarearectangle[0][0]) -100, int(cv2minimumarearectangle[0][1])), \n","                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 30, 255, 255), 3)\n","\n","  return draw_min_area_rect"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5On3YoXHv5nq"},"source":["#Code cell #12\n","largest_min_area = draw_min_area_rect(largest_min_area_rect, dilate)\n","cv2_imshow(largest_min_area)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BcaNsCwAoGjD"},"source":["### 3.d - Back to the actual deskewing routine\n","Getting the angle of the rectangle's skew is actually easier than drawing the rectangle: we just need the last number produced by `minAreaRect`, but because of the way `minAreaRect` measures the angle, we do need to have a bit of math to make sure the value comes out usable."]},{"cell_type":"code","metadata":{"id":"xefaAsQKwNCs"},"source":["#Code cell #13\n","# Determine the angle. Convert it to the value that was originally used to obtain skewed image\n","largest_rect_angle = largest_min_area_rect[-1]\n","if largest_rect_angle < -45:\n","    largest_rect_angle = 90 + largest_rect_angle\n","print(largest_rect_angle)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y7PMczhaqF9b"},"source":["### 3.e - Let's see the deskewed image.\n","The code in the next cell takes a few steps to rotate our image \n","1. First, we make a copy of our original image\n","2. Next, we determine the size of the image by getting its height and width (the first two items returned by `shape`)\n","3. Then, we determine the center of the image by dividing its height and width by two.\n","4. Next, we construct the rotation we want to happen: rotating the image around its center point by the `angle` we determined in the previous cell.\n","\n","Note how we're using information that we calculated by using what is to us a very strange-looking image, and applying it to our original color image."]},{"cell_type":"code","metadata":{"id":"9_6puKgfwm6j"},"source":["#Code cell #14\n","largest_rect_deskew = im.copy()\n","(h, w) = largest_rect_deskew.shape[:2]\n","center = (w // 2, h // 2)\n","# M = cv2.getRotationMatrix2D(center, angle, 1.0)\n","M = cv2.getRotationMatrix2D(center, largest_rect_angle, 1.0)\n","deskewed_largest_rect = cv2.warpAffine(largest_rect_deskew, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n","cv2_imshow(deskewed_largest_rect)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vtHHe--X45HF"},"source":["Those results probably represent an improvement, though not necessarily a huge one. (In a longer version of this notebook, I offered an adjustment to Ertuna's method that seemed like it yielded some small but noticeable improvements with early print.) Try going back and changing some of the earlier variables to see how the final output changes. You could also try working on a different image to see how different images respond to the same process."]},{"cell_type":"markdown","metadata":{"id":"e-W2pJFZd8qb"},"source":["## 4 - Clear Google Colab environment"]},{"cell_type":"code","metadata":{"id":"5-T0W4VLeAS2"},"source":["#Code cell #15\n","%cd /content/\n","!rm -r ./*"],"execution_count":null,"outputs":[]}]}